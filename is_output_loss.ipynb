{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the output function\n",
    "all_im_embs = CLIPModelOutput.image_embeddings\n",
    "all_txt_embs = CLIPModelOutput.text_embeddings\n",
    "N = CLIPModelOutput.num_computed_embeddings\n",
    "sim_bs = CLIPModelOutput.sim_batch_size\n",
    "\n",
    "if all_im_embs is None:\n",
    "    raise AssertionError(\n",
    "        \"Run traker.task.get_embeddings first before featurizing!\"\n",
    "    )\n",
    "\n",
    "# tailored for open_clip\n",
    "# https://github.com/mlfoundations/open_clip/blob/fb72f4db1b17133befd6c67c9cf32a533b85a321/src/open_clip/model.py#L242-L245\n",
    "clip_inputs = {\"image\": image.unsqueeze(0), \"text\": label.unsqueeze(0)}\n",
    "image_embeddings, text_embeddings, _ = ch.func.functional_call(\n",
    "    model, (weights, buffers), args=(), kwargs=clip_inputs\n",
    ")\n",
    "\n",
    "ii = ch.multinomial(\n",
    "    input=ch.arange(N).float(), num_samples=sim_bs, replacement=False\n",
    ")\n",
    "\n",
    "result = -ch.logsumexp(\n",
    "    -image_embeddings @ (text_embeddings - all_txt_embs[ii]).T, dim=1\n",
    ") + -ch.logsumexp(\n",
    "    -text_embeddings @ (image_embeddings - all_im_embs[ii]).T, dim=1\n",
    ")\n",
    "return result.sum()  # shape of result should be [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip loss\n",
    "\n",
    "# image_encoder - ResNet or Vision Transformer\n",
    "# text_encoder - CBOW or Text Transformer\n",
    "# I[n, h, w, c] - minibatch of aligned images\n",
    "# T[n, l] - minibatch of aligned texts\n",
    "# W_i[d_i, d_e] - learned proj of image to embed\n",
    "# W_t[d_t, d_e] - learned proj of text to embed\n",
    "# t - learned temperature parameter\n",
    "\n",
    "# extract feature representations of each modality\n",
    "I_f = image_encoder(I)  # [n, d_i]\n",
    "T_f = text_encoder(T)  # [n, d_t]\n",
    "# joint multimodal embedding [n, d_e]\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
    "# scaled pairwise cosine similarities [n, n]\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
    "# symmetric loss function\n",
    "labels = np.arange(n)\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "loss = (loss_i + loss_t) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "all_img_embds = torch.randn(3, 25)\n",
    "all_txt_embds = torch.randn(3, 25)\n",
    "input_img_emb = torch.randn(1, 25)\n",
    "input_txt_emb = torch.randn(1, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def out_fn(all_img_embds, all_txt_embds, input_img_emb, input_txt_emb):\n",
    "    N = all_img_embds.shape[0]\n",
    "    ii = torch.arange(N)\n",
    "    # torch.multinomial(\n",
    "    #     input=torch.arange(N).float(), num_samples=sim_bs, replacement=False\n",
    "    # )\n",
    "    return -torch.logsumexp(\n",
    "        -input_img_emb @ (input_txt_emb - all_txt_embds[ii]).T, dim=1\n",
    "    ) + -torch.logsumexp(\n",
    "        -input_txt_emb @ (input_img_emb - all_img_embds[ii]).T, dim=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.2665])\n",
      "tensor([-0.0601])\n"
     ]
    }
   ],
   "source": [
    "print(out_fn(all_img_embds, all_txt_embds, input_img_emb, input_txt_emb))\n",
    "print(out_fn(all_img_embds, all_txt_embds, all_img_embds[:1], all_txt_embds[:1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(all_img_embds, all_txt_embds, input_img_emb, input_txt_emb):\n",
    "    # Compute similarity between input image embedding and all text embeddings\n",
    "    logits = torch.matmul(input_img_emb, all_txt_embds.T)\n",
    "    \n",
    "    # Ground truth label (single element for input embeddings)\n",
    "    labels = torch.tensor([0], device=input_img_emb.device)\n",
    "    \n",
    "    # Compute negative log likelihood loss\n",
    "    loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5377)\n",
      "tensor(0.0468)\n"
     ]
    }
   ],
   "source": [
    "print(compute_loss(all_img_embds, all_txt_embds, input_img_emb, input_txt_emb))\n",
    "print(compute_loss(all_img_embds, all_txt_embds, all_img_embds[:1], all_txt_embds[:1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1029) tensor(5.6286)\n",
      "tensor(2.8657)\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(all_img_embds, all_txt_embds, input_img_emb, input_txt_emb):\n",
    "    # Compute similarity between input image embedding and all text embeddings\n",
    "    logits_img = torch.matmul(input_img_emb, all_txt_embds.T)\n",
    "    # Compute similarity between input text embedding and all image embeddings\n",
    "    logits_txt = torch.matmul(input_txt_emb, all_img_embds.T)\n",
    "\n",
    "    # Ground truth labels (single element for input embeddings)\n",
    "    labels_img = torch.tensor([0], device=input_img_emb.device)\n",
    "    labels_txt = torch.tensor([0], device=input_txt_emb.device)\n",
    "\n",
    "    # Compute cross entropy loss for input image and text embeddings\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits_img, labels_img)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits_txt, labels_txt)\n",
    "\n",
    "    return loss_i, loss_t\n",
    "\n",
    "loss_i, loss_t = loss_fn(all_img_embds, all_txt_embds, input_img_emb, input_txt_emb)\n",
    "\n",
    "print(loss_i, loss_t)\n",
    "print((loss_i + loss_t) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curate",
   "language": "python",
   "name": "curate"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
