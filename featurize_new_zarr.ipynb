{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=7\n",
      "Config(device='cuda',\n",
      "       worker_id=0,\n",
      "       worker_total=1,\n",
      "       dry_run=False,\n",
      "       debug=False,\n",
      "       output_dir='/raid/pdpl/trak/grads/',\n",
      "       save_dir='/raid/pdpl/trak/trak_results/',\n",
      "       s3_endpoint_url='https://s3.fraunhofer.de',\n",
      "       write_chunks=1000,\n",
      "       seed=42,\n",
      "       proj_dim=2048,\n",
      "       num_contrastive_samples=50000,\n",
      "       datasets={'CIFAR100': DatasetConfig(uri='/datasets/cifar100/shards/cifar100-train-{000000..000049}.tar',\n",
      "                                           uris=None,\n",
      "                                           size=None,\n",
      "                                           num_workers=16,\n",
      "                                           splittable=True,\n",
      "                                           custom=True),\n",
      "                 'Food101': DatasetConfig(uri='/datasets/food101/shards/food101-train-{000000..000075}.tar',\n",
      "                                          uris=None,\n",
      "                                          size=None,\n",
      "                                          num_workers=16,\n",
      "                                          splittable=True,\n",
      "                                          custom=True),\n",
      "                 'STL10': DatasetConfig(uri='/datasets/stl10/shards/stl10-train-{000000..000004}.tar',\n",
      "                                        uris=None,\n",
      "                                        size=None,\n",
      "                                        num_workers=16,\n",
      "                                        splittable=True,\n",
      "                                        custom=True),\n",
      "                 'commonpool': DatasetConfig(uri='/datasets/datacomp/shards/{00000000..00001287}.tar',\n",
      "                                             uris=None,\n",
      "                                             size=None,\n",
      "                                             num_workers=16,\n",
      "                                             splittable=True,\n",
      "                                             custom=False),\n",
      "                 'fairvision/AMD': DatasetConfig(uri='/datasets/fairvision/AMD/shards/amd-train-{000000..000005}.tar',\n",
      "                                                 uris=None,\n",
      "                                                 size=None,\n",
      "                                                 num_workers=16,\n",
      "                                                 splittable=True,\n",
      "                                                 custom=True),\n",
      "                 'fairvision/DR': DatasetConfig(uri='/datasets/fairvision/DR/shards/dr-train-{000000..000005}.tar',\n",
      "                                                uris=None,\n",
      "                                                size=None,\n",
      "                                                num_workers=16,\n",
      "                                                splittable=True,\n",
      "                                                custom=True),\n",
      "                 'fairvision/Glaucoma': DatasetConfig(uri='/datasets/fairvision/Glaucoma/shards/glaucoma-train-{000000..000005}.tar',\n",
      "                                                      uris=None,\n",
      "                                                      size=None,\n",
      "                                                      num_workers=16,\n",
      "                                                      splittable=True,\n",
      "                                                      custom=True),\n",
      "                 'fitzpatrick17k': DatasetConfig(uri='/datasets/fitzpatrick17k/shards/fitzpatrick17k-train-{000000..000012}.tar',\n",
      "                                                 uris=None,\n",
      "                                                 size=None,\n",
      "                                                 num_workers=16,\n",
      "                                                 splittable=True,\n",
      "                                                 custom=True),\n",
      "                 'pcam': DatasetConfig(uri='/datasets/pcam/shards/pcam-train-{000000..000262}.tar',\n",
      "                                       uris=None,\n",
      "                                       size=None,\n",
      "                                       num_workers=16,\n",
      "                                       splittable=True,\n",
      "                                       custom=True)},\n",
      "       experiments=[ExperimentConfig(name='cifar100',\n",
      "                                     ood_dataset_name='commonpool',\n",
      "                                     id_dataset_name='cifar100',\n",
      "                                     target_datasets=['commonpool', 'cifar100'],\n",
      "                                     encoders=[EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='cifar100_v0',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/cifar100/ratio_1.0/datacomp_v0/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=0),\n",
      "                                               EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='cifar100_v1',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/cifar100/ratio_1.0/datacomp_v1/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=1),\n",
      "                                               EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='cifar100_v2',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/cifar100/ratio_1.0/datacomp_v2/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=2)]),\n",
      "                    ExperimentConfig(name='food101',\n",
      "                                     ood_dataset_name='commonpool',\n",
      "                                     id_dataset_name='food101',\n",
      "                                     target_datasets=['commonpool', 'food101'],\n",
      "                                     encoders=[EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='food101_v0',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/food101/ratio_1.0/datacomp_v0/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=0),\n",
      "                                               EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='food101_v1',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/food101/ratio_1.0/datacomp_v1/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=1),\n",
      "                                               EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='food101_v2',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/food101/ratio_1.0/datacomp_v2/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=2)]),\n",
      "                    ExperimentConfig(name='pcam',\n",
      "                                     ood_dataset_name='commonpool',\n",
      "                                     id_dataset_name='pcam',\n",
      "                                     target_datasets=['commonpool', 'pcam'],\n",
      "                                     encoders=[EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='pcam_v0',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/pcam/ratio_1.0/datacomp_v0/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=0),\n",
      "                                               EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='pcam_v1',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/pcam/ratio_1.0/datacomp_v1/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=1),\n",
      "                                               EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='pcam_v2',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/pcam/ratio_1.0/datacomp_v2/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=2)]),\n",
      "                    ExperimentConfig(name='fairvision/AMD',\n",
      "                                     ood_dataset_name='commonpool',\n",
      "                                     id_dataset_name='fairvision/AMD',\n",
      "                                     target_datasets=['commonpool',\n",
      "                                                      'fairvision/AMD'],\n",
      "                                     encoders=[EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='fairvision/AMD_v0',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/fairvision/AMD/ratio_1.0/datacomp_v0/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=0),\n",
      "                                               EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='fairvision/AMD_v1',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/fairvision/AMD/ratio_1.0/datacomp_v1/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=1),\n",
      "                                               EncoderConfig(architecture='ViT-B-32',\n",
      "                                                             name='fairvision/AMD_v2',\n",
      "                                                             path='s3://pdpl/small_clip_checkpoints/curation/image-based/fairvision/AMD/ratio_1.0/datacomp_v2/small_scale/checkpoints/epoch_5.pt',\n",
      "                                                             url=None,\n",
      "                                                             precision='pure_fp16',\n",
      "                                                             embedding_batch_size=2048,\n",
      "                                                             grad_batch_size=48,\n",
      "                                                             model_id=2)])])\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=7\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pyarrow.dataset as ds\n",
    "import torch as ch\n",
    "import zarr\n",
    "from torch import Tensor\n",
    "from tqdm.rich import tqdm, trange\n",
    "\n",
    "from config.config import Config, ExperimentConfig\n",
    "\n",
    "cfg = Config()\n",
    "# cfg.device=\"cpu\"\n",
    "pprint(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.experiments = [ExperimentConfig(name=\"raw\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.data import uid_int_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ood_grads(experiment_cfg, encoder_cfg):\n",
    "    input_path = str(\n",
    "        Path(cfg.output_dir)\n",
    "        / experiment_cfg.name\n",
    "        / encoder_cfg.name\n",
    "        / experiment_cfg.ood_dataset_name\n",
    "        / \"data.zarr\"\n",
    "    )\n",
    "    dataset = zarr.open(input_path)\n",
    "    if DEBUG:\n",
    "        uids = uid_int_to_str(dataset[\"uid\"][:1000])\n",
    "        g = dataset[\"grads\"][:1000]\n",
    "        out_to_loss = dataset[\"loss_grads\"][:1000]\n",
    "    else:\n",
    "        uids = uid_int_to_str(dataset[\"uid\"][:])\n",
    "        g = dataset[\"grads\"][:]\n",
    "        out_to_loss = dataset[\"loss_grads\"][:]\n",
    "    dtype = [\n",
    "        (\"uids\", uids.dtype),\n",
    "        (\"grads\", g.dtype, g.shape[1]),\n",
    "        (\n",
    "            \"loss_grads\",\n",
    "            out_to_loss.dtype,\n",
    "        ),\n",
    "    ]\n",
    "    combined = np.empty(len(uids), dtype=dtype)\n",
    "    combined[\"uids\"] = uids\n",
    "    combined[\"grads\"] = g\n",
    "    combined[\"loss_grads\"] = out_to_loss\n",
    "\n",
    "    # Sort in-place based on uids\n",
    "    combined.sort(order=\"uids\")\n",
    "\n",
    "    # Extract back the sorted arrays\n",
    "    uids = combined[\"uids\"]\n",
    "    g = ch.tensor(\n",
    "        np.ascontiguousarray(combined[\"grads\"]), device=\"cpu\"\n",
    "    ).pin_memory()\n",
    "    out_to_loss = ch.tensor(\n",
    "        np.ascontiguousarray(combined[\"loss_grads\"]), device=\"cpu\"\n",
    "    ).pin_memory()\n",
    "    return uids, g, out_to_loss\n",
    "\n",
    "\n",
    "def load_dataset_size(experiment_cfg, encoder_cfg):\n",
    "    if DEBUG:\n",
    "        return 1000\n",
    "    input_path = str(\n",
    "        Path(cfg.output_dir)\n",
    "        / experiment_cfg.name\n",
    "        / encoder_cfg.name\n",
    "        / experiment_cfg.ood_dataset_name\n",
    "        / \"data.zarr\"\n",
    "    )\n",
    "    dataset = zarr.open(input_path)\n",
    "    return dataset[\"grads\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xtx(grads: Tensor, batch_size=20_000, progress=None) -> Tensor:\n",
    "    proj_dim = grads.shape[1]\n",
    "    result = ch.zeros(proj_dim, proj_dim, dtype=grads.dtype, device=\"cuda\")\n",
    "    blocks = ch.split(grads, split_size_or_sections=batch_size, dim=0)\n",
    "\n",
    "    # Use progress.track if progress bar is provided, otherwise use regular iteration\n",
    "    iterator = (\n",
    "        progress.track(blocks, description=\"Computing XTX\")\n",
    "        if progress\n",
    "        else blocks\n",
    "    )\n",
    "    for block in iterator:\n",
    "        result += block.T.to(\"cuda\") @ block.to(\"cuda\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_xtx_inv(\n",
    "    grads: Tensor,\n",
    "    xtx: Tensor,\n",
    "    lambda_reg=0.0,\n",
    "    batch_size=20_000,\n",
    "    progress=None,\n",
    ") -> Tensor:\n",
    "    xtx_reg = xtx + lambda_reg * ch.eye(\n",
    "        xtx.size(dim=0), device=xtx.device, dtype=xtx.dtype\n",
    "    )\n",
    "    xtx_inv = ch.linalg.inv(xtx_reg.to(ch.float32))\n",
    "\n",
    "    # center X^TX inverse a bit to avoid numerical issues when going to float16\n",
    "    xtx_inv /= xtx_inv.abs().mean()\n",
    "    xtx_inv = xtx_inv.to(grads.dtype)\n",
    "\n",
    "    grads_blocks = ch.split(grads, split_size_or_sections=batch_size, dim=0)\n",
    "\n",
    "    # Move xtx_inv to GPU once before the loop\n",
    "    xtx_inv_gpu = xtx_inv.cuda()\n",
    "\n",
    "    # Process blocks on GPU\n",
    "    result_blocks = []\n",
    "    # Use progress.track if progress bar is provided\n",
    "    iterator = (\n",
    "        progress.track(grads_blocks, description=\"Processing blocks\")\n",
    "        if progress\n",
    "        else grads_blocks\n",
    "    )\n",
    "    for block in iterator:\n",
    "        block_gpu = block.cuda()\n",
    "        result_gpu = block_gpu @ xtx_inv_gpu\n",
    "        result_blocks.append(result_gpu.cpu())\n",
    "\n",
    "    # Concatenate results on CPU\n",
    "    result = ch.cat(result_blocks)\n",
    "\n",
    "    return result.to(dtype=grads.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(target, id: bool = True):\n",
    "    id_indices_zarr = zarr.open(\"/raid/pdpl/id_downstream_idx.zarr\", mode=\"r\")\n",
    "    if id:\n",
    "        return id_indices_zarr[target][\"id_indices\"]\n",
    "    else:\n",
    "        return id_indices_zarr[target][\"downstream_indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = load_dataset_size(\n",
    "    cfg.experiments[0], cfg.experiments[0].encoders[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "    # \"fitzpatrick17k\",\n",
    "    # \"fairvision/dr\",\n",
    "    \"fairvision/amd\",\n",
    "    # \"fairvision/glaucoma\",\n",
    "    # \"pcam\",\n",
    "    # \"food101\",\n",
    "    # \"cifar100\",\n",
    "    # \"stl10\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.progress import (\n",
    "    BarColumn,\n",
    "    Progress,\n",
    "    SpinnerColumn,\n",
    "    TextColumn,\n",
    "    TimeElapsedColumn,\n",
    ")\n",
    "\n",
    "avg_out_to_loss = ch.zeros(train_dataset_size, device=\"cpu\")\n",
    "avg_scores = {k: ch.zeros(train_dataset_size, device=\"cpu\") for k in targets}\n",
    "\n",
    "with Progress(\n",
    "    SpinnerColumn(),\n",
    "    TextColumn(\"[progress.description]{task.description}\"),\n",
    "    BarColumn(),\n",
    "    TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "    TimeElapsedColumn(),\n",
    ") as progress:\n",
    "    encoder_task = progress.add_task(\n",
    "        \"Processing encoders...\", total=len(cfg.experiments[0].encoders)\n",
    "    )\n",
    "\n",
    "    for encoder_cfg in cfg.experiments[0].encoders:\n",
    "        # all_scores = {}\n",
    "        uids, g, out_to_loss = load_ood_grads(cfg.experiments[0], encoder_cfg)\n",
    "        avg_out_to_loss += out_to_loss\n",
    "        xtx = get_xtx(ch.tensor(g, device=\"cpu\"), progress=progress)\n",
    "        x_xtx_inv = get_x_xtx_inv(\n",
    "            ch.tensor(g, device=\"cpu\"), xtx, progress=progress\n",
    "        )\n",
    "        features = x_xtx_inv.pin_memory()\n",
    "\n",
    "        target_task = progress.add_task(\n",
    "            f\"Processing targets for {encoder_cfg.name}...\", total=len(targets)\n",
    "        )\n",
    "\n",
    "        for target in targets:\n",
    "            input_path = str(\n",
    "                Path(cfg.output_dir)\n",
    "                / cfg.experiments[0].name\n",
    "                / encoder_cfg.name\n",
    "                / target\n",
    "            )\n",
    "            dataset_target = ds.dataset(input_path, format=\"parquet\")\n",
    "            batch_size = 16384\n",
    "            scanner = dataset_target.scanner(\n",
    "                columns=[\"grads\", \"uid\"], batch_size=batch_size\n",
    "            )\n",
    "            batches = scanner.to_batches()\n",
    "            grads_list = []\n",
    "            uids_list = []\n",
    "\n",
    "            batch_task = progress.add_task(\n",
    "                f\"Loading batches for {target}...\",\n",
    "                total=dataset_target.count_rows() // batch_size,\n",
    "            )\n",
    "\n",
    "            for batch in scanner.to_batches():\n",
    "                grads_list.extend(\n",
    "                    batch.column(\"grads\").to_numpy(zero_copy_only=False)\n",
    "                )\n",
    "                uids_list.extend(\n",
    "                    batch.column(\"uid\").to_numpy(zero_copy_only=False)\n",
    "                )\n",
    "                progress.advance(batch_task)\n",
    "\n",
    "            progress.remove_task(batch_task)\n",
    "\n",
    "            g_target = np.stack(grads_list)\n",
    "            uids_target = np.stack(uids_list)\n",
    "            dtype = [\n",
    "                (\"uids\", uids_target.dtype),\n",
    "                (\"grads\", g_target.dtype, g_target.shape[1]),\n",
    "            ]\n",
    "            combined = np.empty(len(uids_target), dtype=dtype)\n",
    "            combined[\"uids\"] = uids_target\n",
    "            combined[\"grads\"] = g_target\n",
    "            combined.sort(order=\"uids\")\n",
    "            uids_target = combined[\"uids\"]\n",
    "            g_target = combined[\"grads\"]\n",
    "            id_indices = get_indices(\n",
    "                target, id=False\n",
    "            )  # get downstream indices\n",
    "            g_target_pt = ch.tensor(\n",
    "                g_target[id_indices], device=\"cpu\"\n",
    "            ).pin_memory()\n",
    "\n",
    "            batch_size = 8192 * 2\n",
    "            scores = []\n",
    "\n",
    "            score_task = progress.add_task(\n",
    "                f\"Computing scores for {target}...\",\n",
    "                total=len(features) // batch_size + 1,\n",
    "            )\n",
    "\n",
    "            for i in range(0, len(features), batch_size):\n",
    "                batch = features[i : i + batch_size].cuda()\n",
    "                batch_scores = ch.mean(batch @ g_target_pt.cuda().T, axis=1)\n",
    "                scores.append(batch_scores.cpu())\n",
    "                progress.advance(score_task)\n",
    "\n",
    "            progress.remove_task(score_task)\n",
    "            scores = ch.cat(scores)\n",
    "            avg_scores[target] += scores\n",
    "            # all_scores[target] = scores.cpu()\n",
    "            progress.advance(target_task)\n",
    "\n",
    "        progress.remove_task(target_task)\n",
    "        progress.advance(encoder_task)\n",
    "\n",
    "avg_out_to_loss /= len(cfg.experiments[0].encoders)\n",
    "avg_scores = {\n",
    "    k: v / len(cfg.experiments[0].encoders) for k, v in avg_scores.items()\n",
    "}\n",
    "final_scores = {k: v * avg_out_to_loss for k, v in avg_scores.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores[\"fairvision/amd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_zarr = zarr.open(\n",
    "    \"/datasets/datacomp/nearest_neighbor_scores.zarr\", mode=\"a\"\n",
    ")\n",
    "if \"trak\" not in scores_zarr:\n",
    "    scores_zarr.create_group(\"trak\")\n",
    "scores_zarr = scores_zarr[\"trak\"]\n",
    "# get target features\n",
    "for target in [\n",
    "    \"fitzpatrick17k\",\n",
    "    \"fairvision/dr\",\n",
    "    \"fairvision/amd\",\n",
    "    \"fairvision/glaucoma\",\n",
    "    \"pcam\",\n",
    "    \"food101\",\n",
    "    \"cifar100\",\n",
    "    \"stl10\",\n",
    "]:\n",
    "    if target in all_scores.keys():\n",
    "        continue\n",
    "    if target in scores_zarr:\n",
    "        all_scores[target] = scores_zarr[target][\"id_scores\"]\n",
    "        continue\n",
    "    input_path = str(Path(cfg.output_dir) / encoder_cfg.name / target)\n",
    "    dataset_target = ds.dataset(input_path, format=\"parquet\")\n",
    "    batch_size = 16384\n",
    "    scanner = dataset_target.scanner(\n",
    "        columns=[\"grads\", \"uid\"], batch_size=batch_size\n",
    "    )\n",
    "    batches = scanner.to_batches()\n",
    "    grads_list = []\n",
    "    uids_list = []\n",
    "    for batch in tqdm(\n",
    "        scanner.to_batches(), total=dataset_target.count_rows() // batch_size\n",
    "    ):\n",
    "        grads_list.extend(batch.column(\"grads\").to_numpy(zero_copy_only=False))\n",
    "        uids_list.extend(batch.column(\"uid\").to_numpy(zero_copy_only=False))\n",
    "    g_target = np.stack(grads_list)\n",
    "    uids_target = np.stack(uids_list)\n",
    "    dtype = [\n",
    "        (\"uids\", uids_target.dtype),\n",
    "        (\"grads\", g_target.dtype, g_target.shape[1]),\n",
    "    ]\n",
    "    combined = np.empty(len(uids_target), dtype=dtype)\n",
    "    combined[\"uids\"] = uids_target\n",
    "    combined[\"grads\"] = g_target\n",
    "    combined.sort(order=\"uids\")\n",
    "    uids_target = combined[\"uids\"]\n",
    "    g_target = combined[\"grads\"]\n",
    "    id_indices = get_indices(target, id=False)  # get downstream indices\n",
    "    g_target_pt = ch.tensor(g_target[id_indices], device=\"cpu\").pin_memory()\n",
    "\n",
    "    batch_size = 8192 * 2\n",
    "    scores = []\n",
    "    for i in trange(0, len(features), batch_size):\n",
    "        batch = features_pt[i : i + batch_size].cuda()\n",
    "        batch_scores = ch.mean(batch @ g_target_pt.cuda().T, axis=1)\n",
    "        scores.append(batch_scores.cpu())\n",
    "    scores = ch.cat(scores)\n",
    "    scores = scores * out_to_loss\n",
    "    if target not in scores_zarr:\n",
    "        target_group = scores_zarr.create_group(target)\n",
    "    else:\n",
    "        target_group = scores_zarr[target]\n",
    "\n",
    "    # Save the scores\n",
    "    target_group.array(\n",
    "        \"id_scores\", np.array(scores.cpu()), dtype=np.float32, overwrite=True\n",
    "    )\n",
    "    all_scores[target] = scores.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for key, scores in all_scores.items():\n",
    "    mean = scores.mean().item()\n",
    "    std = scores.std().item()\n",
    "    data.append({\"dataset\": key, \"mean\": mean, \"std\": std})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sort_values(\"std\", ascending=False)\n",
    "print(df.to_string(float_format=lambda x: \"{:.4f}\".format(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\", {\"grid.alpha\": 0.3})\n",
    "\n",
    "# Create a continuous color palette with enough colors\n",
    "num_datasets = len(all_scores)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_datasets))\n",
    "\n",
    "\n",
    "def compute_histogram(data, bins=50):\n",
    "    hist, bin_edges = jnp.histogram(data, bins=bins)\n",
    "    return hist, bin_edges\n",
    "\n",
    "\n",
    "# Plot in order of standard deviation from df\n",
    "for i, row in enumerate(df.itertuples()):\n",
    "    key = row.dataset\n",
    "    scores = all_scores[key]\n",
    "    scores_np = jnp.array(scores.numpy())\n",
    "    hist, bins = compute_histogram(scores_np)\n",
    "\n",
    "    # Convert back to numpy for seaborn plotting\n",
    "    hist = hist.block_until_ready()\n",
    "    sns.lineplot(x=bins[:-1], y=hist, alpha=0.5, label=key, color=colors[i])\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "plt.title(\"Distribution of Scores Across Datasets\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trak.utils import get_matrix_mult_blockwise\n",
    "\n",
    "# full_scores = get_matrix_mult_blockwise(\n",
    "#     features, ch.tensor(g_target, device=\"cpu\"), ch.float16, bs=2048\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
